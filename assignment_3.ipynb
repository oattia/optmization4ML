{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 3\n",
    "\n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and answers to the mathematical questions on Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, scikit-image, skimage, matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data.astronaut()\n",
    "img = rgb2gray(img)*255 # convert to gray and change scale from (0,1) to (0,255).\n",
    "\n",
    "n = img.shape[0]\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the differences operators here. Use your code from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need these three methods to construct sparse differences operators.\n",
    "# If you do not use sparse operators you might have scalability problems.\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "\n",
    "# Use your code from Assignment 2. \n",
    "# Make sure that you compute the right D_h and D_v matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = 0\n",
    "standard_deviation = 30\n",
    "dimensions = (n,n)\n",
    "\n",
    "noise = np.random.normal(mean_,standard_deviation,dimensions)\n",
    "\n",
    "noisy_image = img + noise\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(noisy_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (8 marks): implement gradient descent with Armijo line-search for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with Armijo line-search to denoise the image. Parameter tunning is not given for this assignment. You will have to tune all parameters yourself. Regarding the quality of the output image, pick the $\\lambda$ parameter that makes the error $$\\frac{1}{n^2}\\|z_{output} - z_{clean}\\|_2$$ as small as possible, where $z_{output}$ is the output of the algorithm. Find $\\lambda$ by trial and error. Note that the smoothing parameter $\\mu$ affects the quality of the output as well. Pick $\\mu$ small enough such that the above error does not improve much for smaller values of $\\mu$. I will measure the running time only for your chosen parameters $\\lambda$ and $\\mu$, therefore, make sure to seperate any code that does trial and error and the code that reports the result for the chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5 marks): implement gradient descent with simple line-search for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call gradient descent with simple line-search to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (6 marks): Compute a Lipschitz constant for the smoothed Total-Variation problem. Note the Lipschitz constant is not unique. However, the minimum Lipschitz constant will give you better performance in algorithms compared to larger Lipschitz constants. This means that whatever you compute here will affect the running time of your algorithm in Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (8 marks): implement accelerated gradient for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. Use the Lipschitz constant that you obtained in Q3. Do not include computation of the Lipschitz constant in this question. You can do it in Q3 and the time for computing the Lipschitz constant will not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call accelerated gradient to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 marks): in Q4 you were asked to implement accelerated gradient by using constant step-sizes $1/L.$ However, computing the Lipschitz constant might take a lot of time and it often results in slow convergence because the step-sizes are too small. Below I give you a practical accelerated method that does not require knowing the Lipschitz constant. The step-sizes $$\\alpha_k$$ in this algorithm can be computed using Armijo line-search. Implement this algorithm for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. \n",
    "\n",
    "Step 1) Choose an $x_0$ and set $y_1=x_0$, $t_1=1$.\n",
    "\n",
    "Step 2) Repeat the following steps until $\\|\\nabla f(x_k)\\|_2\\le \\epsilon$\n",
    "\n",
    "Step 3) Compute $\\alpha_k$ using Armijo line-search. Armijo line-search should be measured at $y_k-\\alpha_k \\nabla f(y_k)$ (as the next point) and $y_k$ (as the current point).\n",
    "\n",
    "Step 4) Set $$x_{k}=y_k - \\alpha_k \\nabla f(y_k)$$\n",
    "\n",
    "Step 5) Set $$t_{k+1}=\\frac{1 + \\sqrt{1+4t_k^2}}{2}$$\n",
    "\n",
    "Step 6) Set $$y_{k+1}=x_k + \\frac{t_k-1}{t_{k+1}}(x_k-x_{k-1})$$\n",
    "\n",
    "Reference: this algorithm is given in \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\" by A. Beck and M. Teboulle.\n",
    "===================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the practical accelerated gradient to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (5 marks): Compare all the methods that you implemented above. Make a plot where in the y-axis is the value of the smoothed objective function and in the x-axis the number of iterations. Compare the methods using the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (8 marks): Illustrate the trade-off between the number of iterations and the smoothing parameter $\\mu$ for gradient descent with Armijo line-search and accelerated gradient with Armijo line-search. Do this by plotting the number of iterations (y-axis) vs magnitude of parameter $\\mu$ (x-axis in ascending order). Start from a small $\\mu$ and increase it gradually. Plot the result for both methods in the same plot. Use appropriate legends for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (5 marks): Derive a smooth approximation (Huber function) of the L1-norm using the convex conjugate of the L1-norm and the distance function $$d(y)=\\frac{1}{2}\\|y\\|_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 9 (7 marks): Derive the pseudo-Huber function using the convex conjugate of the L1-norm.  To derive the pseudo-Huber use the distance function $$d(y)=\\sum_{i=1}^n 1 - \\sqrt{1-y_i^2}$$ with domain $$|y_i|\\le 1 \\ \\ \\forall i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 10 (3 marks): Show that the pseudo-Huber function is convex and not strongly-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Question 11 (7 marks): If the convex conjugate of $f$ is strongly-convex with compact convex domain then $$\\max_{y \\in \\mbox{dom} \\ f^*} x^Ty - f^*(y)$$ has a unique maximizer. Show that  $$\\nabla f(x) =\\mbox{argmax}_{y \\in \\mbox{dom} \\ f^*} x^Ty - f^*(y).$$ Hint: one approach to solve this is to lower and upper bound $f$ and then use the definition of directional derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Question 12 (7 marks): Prove that if the convex conjugate of $f$ is $\\delta$-strongly-convex, then $\\nabla f(x)$ is Lipschitz continuous with Lipschitz constant $1/\\delta.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 (8 marks):  Assume that the domain of the convex conjugate satisfies $$\\mbox{dom} \\ f^* \\subseteq \\mathbb{R}^n.$$ and it is closed and bounded. You are given a continuous distance function $d(y)$ where its domain satisfies $$\\mbox{dom} \\ f^* \\subseteq \\mbox{dom} \\ d .$$ Prove that $$f(x) - \\mu D \\le f_\\mu(x) \\le f(x),$$ where $D$ has to be bounded and $f_\\mu(x)$ is the smooth approximation of $f$. See slides 54-57 in Lecture07and08 on piazza. Hint: you will need the Weierstrass extreme value theorem to show that $D$ is bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## ## Question 14 (8 marks): We proved that for any convex function with Lipschitz continuous gradient there exists and estimate sequence. Show that for an estimate sequence we have that $$f(x_k) - f^* = \\mathcal{O}\\left(\\frac{1}{k^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 (5 marks): Obtain the convergence rate and iteration complexity for the accelerated method for strongly-convex functions with Lipschitz continuous gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
