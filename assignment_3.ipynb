{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 3\n",
    "\n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and answers to the mathematical questions on Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:18:35.756484Z",
     "start_time": "2019-10-24T14:18:34.942566Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, scikit-image, skimage, matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:18:36.148785Z",
     "start_time": "2019-10-24T14:18:35.770323Z"
    }
   },
   "outputs": [],
   "source": [
    "img = data.astronaut()\n",
    "img = rgb2gray(img)*255 # convert to gray and change scale from (0,1) to (0,255).\n",
    "\n",
    "n = img.shape[0]\n",
    "m = n\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the differences operators here. Use your code from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T16:27:04.161349Z",
     "start_time": "2019-10-24T16:27:03.900510Z"
    }
   },
   "outputs": [],
   "source": [
    "# You will need these three methods to construct sparse differences operators.\n",
    "# If you do not use sparse operators you might have scalability problems.\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import real\n",
    "# Use your code from Assignment 2. \n",
    "# Make sure that you compute the right D_h and D_v matrices.\n",
    "\n",
    "J = diags(diagonals=[\n",
    "    [-1.0] * n,\n",
    "    [1.0] * (n-1)\n",
    "], offsets=[0, 1])\n",
    "\n",
    "I = identity(n=n)\n",
    "\n",
    "# Forward Horizontal Difference\n",
    "D_h = kron(J, I)\n",
    "\n",
    "# Forward Vertical Difference\n",
    "D_v = kron(I, J)\n",
    "\n",
    "D = D_h + 1j * D_v\n",
    "\n",
    "D_s = real((D.H).dot(D))\n",
    "I_s = identity(n*m)\n",
    "D_d = D * D.conj()\n",
    "D_x = D_h.T.dot(D_h) + D_v.T.dot(D_v)\n",
    "D_t = (D.H).dot(D)\n",
    "D_f = D_h + D_v\n",
    "\n",
    "# Matrix norm with tolerance parameter to control speed vs. precision tradeoff\n",
    "def matrix_p2_norm(A, tol=0.001):\n",
    "    eig_val = eigsh((A.H).dot(A), return_eigenvectors=False, tol=tol, k=1)\n",
    "    return np.sqrt(eig_val).sum()\n",
    "\n",
    "# Vector norm\n",
    "def vector_norm(x):\n",
    "    return np.sqrt(real((x.H).dot(x))).sum()\n",
    "\n",
    "# The laplacian denoising objective function Hessian\n",
    "def G_lap(lamb):\n",
    "    return lamb * D_s + I_s\n",
    "\n",
    "# The laplacian denoising objective function\n",
    "def f_lap(lamb, x, z_n):\n",
    "    return real(0.5 * lamb * (vector_norm(D.dot(x)) ** 2.0) + 0.5 * vector_norm(x - z_n) ** 2.0)\n",
    "\n",
    "# The laplacian denoising objective function gradient\n",
    "def grad_f_lap(lamb, x, z_n):\n",
    "    return G_lap(lamb).dot(x) - z_n\n",
    "\n",
    "# Psuedo-Huber function for vector v, to replace L1 norm\n",
    "def psf(v, mu):\n",
    "    return np.sqrt(mu ** 2.0  + np.power(np.abs(v), 2.0)) - mu\n",
    "\n",
    "# The total-variation denoising objective function\n",
    "def f_tv(lamb, mu, x, z_n):\n",
    "    return real(lamb * psf(v=D.dot(x), mu=mu).sum() + 0.5 * np.linalg.norm(x - z_n) ** 2.0)\n",
    "\n",
    "# The total-variation denoising objective function gradient\n",
    "def grad_f_tv(lamb, mu, x, z_n):\n",
    "    x_d = D.dot(x)\n",
    "    x_d_abs = np.abs(x_d)\n",
    "    #return lamb * D_d.dot(x) / np.sqrt(mu ** 2.0 + np.power(x_d_abs, 2.0)) + x - z_n\n",
    "    print(D_f.shape)\n",
    "    print(x.shape)\n",
    "    return lamb * (D_f).dot(x) / np.sqrt(mu ** 2.0 + np.power(x_d_abs, 2.0)) + x - z_n\n",
    "\n",
    "\n",
    "# The total-variation denoising objective function gradient\n",
    "def grad_f_tv2(lamb, mu, x, z_n):\n",
    "    x = csr_matrix(x)\n",
    "    \n",
    "    dhx = D_h.dot(x)\n",
    "    dvx = D_v.dot(x)\n",
    "    \n",
    "    numer = D_h.multiply(dhx) + D_v.multiply(dvx)\n",
    "    \n",
    "    d_x_abs = dhx.power(2.0) + dvx.power(2.0)\n",
    "    denom = np.sqrt(mu ** 2.0 + d_x_abs.toarray())\n",
    "    \n",
    "    frac = numer.multiply(1.0 / denom)\n",
    "    return lamb * frac.sum(axis=0).T  + x - z_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:18:37.144198Z",
     "start_time": "2019-10-24T14:18:37.010550Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-0.01, 0.01, step=0.002)\n",
    "y_abs = np.abs(x)\n",
    "y_mu1 = psf(x, mu=0.001)\n",
    "y_mu2 = psf(x, mu=0.01)\n",
    "y_mu3 = psf(x, mu=0.1)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y_abs, 'r--', x, y_mu1, 'g--', x, y_mu2, 'b--', x, y_mu3, 'm--')\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:18:38.488716Z",
     "start_time": "2019-10-24T14:18:38.017396Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_ = 0\n",
    "standard_deviation = 30\n",
    "dimensions = (n,n)\n",
    "\n",
    "noise = np.random.normal(mean_,standard_deviation,dimensions)\n",
    "\n",
    "noisy_image = img + noise\n",
    "noisy_image_vec = csr_matrix(noisy_image.T.ravel()).T\n",
    "image_vec = csr_matrix(img.T.ravel()).T\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(noisy_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (8 marks): implement gradient descent with Armijo line-search for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T16:27:09.937355Z",
     "start_time": "2019-10-24T16:27:09.925368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a line-search-armijo function\n",
    "def line_search_arm(lambda_, mu, x, f_x, grad_f_x, norm_grad_f_x, gamma):\n",
    "    # lambda_: the regularization parameter\n",
    "    # x: the current estimate of t=he variable\n",
    "    # f_x: the value of the objective function at x\n",
    "    # grad_f_x: the gradient of the objective function at x\n",
    "    # norm_grad_f_x: the norm of grad_f_x\n",
    "    # gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "\n",
    "    # Write your code here.\n",
    "    alpha = 1.0\n",
    "    loss = gamma * (norm_grad_f_x ** 2.0)\n",
    "    while f_tv(lamb=lambda_, mu=mu, x=x-alpha*grad_f_x, z_n=noisy_image_vec) > f_x - alpha * loss:\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "    \n",
    "def gradient_descent_arm(x0, epsilon, lambda_, mu, max_iterations, gamma):\n",
    "    # x0: is the initial guess for the x variables\n",
    "    # epsilon: is the termination tolerance parameter\n",
    "    # lambda_: is the regularization parameter of the denoising problem.\n",
    "    # max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "    # gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "    \n",
    "    # Write your code here.\n",
    "    x_updated = x0.copy().toarray()\n",
    "    f_vals = []\n",
    "    norm_vals = []\n",
    "    t1 = time.time()\n",
    "    for i in range(1, max_iterations+1):\n",
    "        current_grad = grad_f_tv2(lamb=lambda_, mu=mu, x=x_updated, z_n=noisy_image_vec)\n",
    "        tx = time.time()\n",
    "        current_grad_norm = np.linalg.norm(current_grad)\n",
    "        if current_grad_norm <= epsilon:\n",
    "            break\n",
    "        norm_vals.append(current_grad_norm)\n",
    "        f_vals.append(f_tv(lamb=lambda_, mu=mu, x=x_updated, z_n=noisy_image_vec))\n",
    "        alpha = line_search_arm(lambda_=lambda_, mu=mu, x=x_updated, f_x=f_vals[-1], grad_f_x=current_grad, norm_grad_f_x=current_grad_norm, gamma=gamma)\n",
    "        x_updated = x_updated - alpha * current_grad\n",
    "        f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "        grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "        print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Norm. Diff. = {grad_diff}\")\n",
    "    t2 = time.time()\n",
    "    print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "    return x_updated, np.array(f_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with Armijo line-search to denoise the image. Parameter tunning is not given for this assignment. You will have to tune all parameters yourself. Regarding the quality of the output image, pick the $\\lambda$ parameter that makes the error $$\\frac{1}{n^2}\\|z_{output} - z_{clean}\\|_2$$ as small as possible, where $z_{output}$ is the output of the algorithm. Find $\\lambda$ by trial and error. Note that the smoothing parameter $\\mu$ affects the quality of the output as well. Pick $\\mu$ small enough such that the above error does not improve much for smaller values of $\\mu$. I will measure the running time only for your chosen parameters $\\lambda$ and $\\mu$, therefore, make sure to seperate any code that does trial and error and the code that reports the result for the chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T16:28:14.367980Z",
     "start_time": "2019-10-24T16:27:16.475677Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 45\n",
    "mu = 0.001\n",
    "epsilon = 1.0e-2\n",
    "gamma = 0.1\n",
    "max_iterations = 200\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here.\n",
    "optimized_gd_arm, f_vals_arm = gradient_descent_arm(x0=noisy_image_vec, \n",
    "                                                    lambda_=lambda_,\n",
    "                                                    mu=mu,\n",
    "                                                    epsilon=epsilon,\n",
    "                                                    gamma=gamma,\n",
    "                                                    max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:51:18.212649Z",
     "start_time": "2019-10-24T14:51:18.194797Z"
    }
   },
   "outputs": [],
   "source": [
    "# mu = 0.01\n",
    "    # 10: 8515.536787930056\n",
    "    # 25: 5403.552568529774\n",
    "    # 30: 5218.644577836724\n",
    "    # 40: 5092.992288363075\n",
    "    # 45: 5080.710291882298\n",
    "    # 50: 5104.336472912059\n",
    "    # 60: 5201.170909978507\n",
    "    # 70: 5349.886968970723\n",
    "# mu = 0.001\n",
    "    # 45: 5080.537317632449\n",
    "vector_norm(optimized_gd_arm - image_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:51:20.775027Z",
     "start_time": "2019-10-24T14:51:20.504857Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "denoised_image_gd_arm = optimized_gd_arm.reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_gd_arm, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5 marks): implement gradient descent with simple line-search for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:56:30.656338Z",
     "start_time": "2019-10-24T14:56:30.644598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a line-search-ls function here.  \n",
    "def line_search_ls(lambda_, mu, x, f_x, grad_f_x):\n",
    "    # lambda_: the regularization parameter\n",
    "    # x: the current estimate of t=he variable\n",
    "    # f_x: the value of the objective function at x\n",
    "    # grad_f_x: the gradient of the objective function at x\n",
    "    \n",
    "    # Write your code here.\n",
    "    alpha = 1.0\n",
    "    while f_tv(lamb=lambda_, mu=mu, x=x-alpha*grad_f_x, z_n=noisy_image_vec) >= f_x:\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "\n",
    "# Write gradient descent + line-search here.\n",
    "def gradient_descent_ls(x0, epsilon, lambda_, mu, max_iterations):\n",
    "    # x0: is the initial guess for the x variables\n",
    "    # epsilon: is the termination tolerance parameter\n",
    "    # lambda_: is the regularization parameter of the denoising problem.\n",
    "    # max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "\n",
    "    # Write your code here.\n",
    "    x_updated = x0.copy().toarray()\n",
    "    f_vals = []\n",
    "    norm_vals = []\n",
    "    t1 = time.time()\n",
    "    for i in range(1, max_iterations+1):\n",
    "        current_grad = grad_f_tv2(lamb=lambda_, mu=mu, x=x_updated, z_n=noisy_image_vec)\n",
    "        current_grad_norm = np.linalg.norm(current_grad)\n",
    "        if current_grad_norm <= epsilon:\n",
    "            break\n",
    "        norm_vals.append(current_grad_norm)\n",
    "        f_vals.append(f_tv(lamb=lambda_, mu=mu, x=x_updated, z_n=noisy_image_vec))\n",
    "        alpha = line_search_ls(lambda_=lambda_, mu=mu, x=x_updated, f_x=f_vals[-1], grad_f_x=current_grad)\n",
    "        x_updated = x_updated - alpha * current_grad\n",
    "        f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "        grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "        print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "    t2 = time.time()\n",
    "    print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "    return x_updated, np.array(f_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call gradient descent with simple line-search to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:58:13.867255Z",
     "start_time": "2019-10-24T14:57:21.029764Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 45\n",
    "mu = 0.001\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 200\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here.\n",
    "optimized_gd_ls, f_vals_ls = gradient_descent_ls(x0=noisy_image_vec,\n",
    "                                                 lambda_=lambda_, \n",
    "                                                 mu=mu,\n",
    "                                                 epsilon=epsilon, \n",
    "                                                 max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T14:58:33.741660Z",
     "start_time": "2019-10-24T14:58:33.467316Z"
    }
   },
   "outputs": [],
   "source": [
    "vector_norm(optimized_gd_ls - image_vec)\n",
    "denoised_image_gd_ls = optimized_gd_ls.reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_gd_ls, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (6 marks): Compute a Lipschitz constant for the smoothed Total-Variation problem. Note the Lipschitz constant is not unique. However, the minimum Lipschitz constant will give you better performance in algorithms compared to larger Lipschitz constants. This means that whatever you compute here will affect the running time of your algorithm in Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T16:36:17.488374Z",
     "start_time": "2019-10-24T16:36:17.484873Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_lipschitz_tv(lamb, mu):\n",
    "    A = (lamb / mu) * (D.H.dot(D)) + I_s\n",
    "    return matrix_p2_norm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T16:36:42.281778Z",
     "start_time": "2019-10-24T16:36:36.190443Z"
    }
   },
   "outputs": [],
   "source": [
    "L = compute_lipschitz_tv(lamb=lambda_, mu=mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (8 marks): implement accelerated gradient for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. Use the Lipschitz constant that you obtained in Q3. Do not include computation of the Lipschitz constant in this question. You can do it in Q3 and the time for computing the Lipschitz constant will not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call accelerated gradient to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 marks): in Q4 you were asked to implement accelerated gradient by using constant step-sizes $1/L.$ However, computing the Lipschitz constant might take a lot of time and it often results in slow convergence because the step-sizes are too small. Below I give you a practical accelerated method that does not require knowing the Lipschitz constant. The step-sizes $$\\alpha_k$$ in this algorithm can be computed using Armijo line-search. Implement this algorithm for the Total-Variation denoising problem. Use the pseudo-Huber function to smooth the problem. \n",
    "\n",
    "Step 1) Choose an $x_0$ and set $y_1=x_0$, $t_1=1$.\n",
    "\n",
    "Step 2) Repeat the following steps until $\\|\\nabla f(x_k)\\|_2\\le \\epsilon$\n",
    "\n",
    "Step 3) Compute $\\alpha_k$ using Armijo line-search. Armijo line-search should be measured at $y_k-\\alpha_k \\nabla f(y_k)$ (as the next point) and $y_k$ (as the current point).\n",
    "\n",
    "Step 4) Set $$x_{k}=y_k - \\alpha_k \\nabla f(y_k)$$\n",
    "\n",
    "Step 5) Set $$t_{k+1}=\\frac{1 + \\sqrt{1+4t_k^2}}{2}$$\n",
    "\n",
    "Step 6) Set $$y_{k+1}=x_k + \\frac{t_k-1}{t_{k+1}}(x_k-x_{k-1})$$\n",
    "\n",
    "Reference: this algorithm is given in \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\" by A. Beck and M. Teboulle.\n",
    "===================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the practical accelerated gradient to denoise the image. Use the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (5 marks): Compare all the methods that you implemented above. Make a plot where in the y-axis is the value of the smoothed objective function and in the x-axis the number of iterations. Compare the methods using the same $\\lambda$ and $\\mu$ that you used in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (8 marks): Illustrate the trade-off between the number of iterations and the smoothing parameter $\\mu$ for gradient descent with Armijo line-search and accelerated gradient with Armijo line-search. Do this by plotting the number of iterations (y-axis) vs magnitude of parameter $\\mu$ (x-axis in ascending order). Start from a small $\\mu$ and increase it gradually. Plot the result for both methods in the same plot. Use appropriate legends for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (5 marks): Derive a smooth approximation (Huber function) of the L1-norm using the convex conjugate of the L1-norm and the distance function $$d(y)=\\frac{1}{2}\\|y\\|_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 9 (7 marks): Derive the pseudo-Huber function using the convex conjugate of the L1-norm.  To derive the pseudo-Huber use the distance function $$d(y)=\\sum_{i=1}^n 1 - \\sqrt{1-y_i^2}$$ with domain $$|y_i|\\le 1 \\ \\ \\forall i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 10 (3 marks): Show that the pseudo-Huber function is convex and not strongly-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Question 11 (7 marks): If the convex conjugate of $f$ is strongly-convex with compact convex domain then $$\\max_{y \\in \\mbox{dom} \\ f^*} x^Ty - f^*(y)$$ has a unique maximizer. Show that  $$\\nabla f(x) =\\mbox{argmax}_{y \\in \\mbox{dom} \\ f^*} x^Ty - f^*(y).$$ Hint: one approach to solve this is to lower and upper bound $f$ and then use the definition of directional derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Question 12 (7 marks): Prove that if the convex conjugate of $f$ is $\\delta$-strongly-convex, then $\\nabla f(x)$ is Lipschitz continuous with Lipschitz constant $1/\\delta.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 (8 marks):  Assume that the domain of the convex conjugate satisfies $$\\mbox{dom} \\ f^* \\subseteq \\mathbb{R}^n.$$ and it is closed and bounded. You are given a continuous distance function $d(y)$ where its domain satisfies $$\\mbox{dom} \\ f^* \\subseteq \\mbox{dom} \\ d .$$ Prove that $$f(x) - \\mu D \\le f_\\mu(x) \\le f(x),$$ where $D$ has to be bounded and $f_\\mu(x)$ is the smooth approximation of $f$. See slides 54-57 in Lecture07and08 on piazza. Hint: you will need the Weierstrass extreme value theorem to show that $D$ is bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 14 (8 marks): We proved that for any convex function with Lipschitz continuous gradient there exists and estimate sequence. Show that for an estimate sequence we have that $$f(x_k) - f^* = \\mathcal{O}\\left(\\frac{1}{k^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 (5 marks): Obtain the convergence rate and iteration complexity for the accelerated method for strongly-convex functions with Lipschitz continuous gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
