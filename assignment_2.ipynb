{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2\n",
    "\n",
    "## The assignment is divided into programming and mathematical questions. Both of them are given in this notebook.\n",
    "\n",
    "## Programming questions: I am giving you a template that you can use to write your code. Description of the questions is integrated in the comments.\n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and answers to the mathematical questions on Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:52:18.735307Z",
     "start_time": "2019-10-11T14:52:18.190863Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, scikit-image, skimage, matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:52:19.834928Z",
     "start_time": "2019-10-11T14:52:19.480482Z"
    }
   },
   "outputs": [],
   "source": [
    "img = data.astronaut()\n",
    "img = rgb2gray(img)*255 # convert to gray and change scale from (0,1) to (0,255).\n",
    "\n",
    "n = img.shape[0]\n",
    "m = n\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the differences operators here. Use your code from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:54:24.646046Z",
     "start_time": "2019-10-11T14:54:24.502002Z"
    }
   },
   "outputs": [],
   "source": [
    "# You will need these three methods to construct sparse differences operators.\n",
    "# If you do not use sparse operators you might have scalability problems.\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import real\n",
    "\n",
    "# Use your code from Assignment 1. \n",
    "# Make sure that you compute the right D_h and D_v matrices.\n",
    "\n",
    "J = diags(diagonals=[\n",
    "    [-1.0] * n,\n",
    "    [1.0] * (n-1)\n",
    "], offsets=[0, 1])\n",
    "\n",
    "I = identity(n=n)\n",
    "\n",
    "# Forward Horizontal Difference\n",
    "D_h = kron(J, I)\n",
    "\n",
    "# Forward Vertical Difference\n",
    "D_v = kron(I, J)\n",
    "\n",
    "D = D_h + 1j * D_v\n",
    "\n",
    "D_s = real((D.H).dot(D))\n",
    "I_s = identity(n*m)\n",
    "\n",
    "\n",
    "# Matrix norm with tolerance parameter to control speed vs. precision tradeoff\n",
    "def matrix_p2_norm(A, tol=0.001):\n",
    "    eig_val = eigsh((A.H).dot(A), return_eigenvectors=False, tol=tol, k=1)\n",
    "    return np.sqrt(eig_val).sum()\n",
    "\n",
    "# Vector norm\n",
    "def vector_norm(x):\n",
    "    return np.sqrt(real((x.H).dot(x))).sum()\n",
    "\n",
    "# The denoising objective function Hessian\n",
    "def G(lamb):\n",
    "    return lamb * D_s + I_s\n",
    "\n",
    "# The denoising objective function\n",
    "def f(lamb, x, z_n):\n",
    "    return real(0.5 * lamb * (vector_norm(D.dot(x)) ** 2.0) + 0.5 * vector_norm(x - z_n) ** 2.0)\n",
    "\n",
    "# The denoising objective function gradient\n",
    "def grad_f(lamb, x, z_n):\n",
    "    return G(lamb).dot(x) - z_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:54:27.675487Z",
     "start_time": "2019-10-11T14:54:27.288305Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_ = 0\n",
    "standard_deviation = 30\n",
    "dimensions = (n,n)\n",
    "\n",
    "noise = np.random.normal(mean_,standard_deviation,dimensions)\n",
    "\n",
    "noisy_image = img + noise\n",
    "noisy_image_vec = csr_matrix(noisy_image.T.ravel()).T\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(noisy_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Question 1: implement gradient descent using the Lipschitz constant as the step-size for the denoising problem. Use eigsh method from scipy.sparse.linalg to compute the Lipschitz constant. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:54:35.551921Z",
     "start_time": "2019-10-11T14:54:35.530250Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_lip(x0, epsilon, lambda_, max_iterations):\n",
    "    # x0: is the initial guess for the x variables\n",
    "    # epsilon: is the termination tolerance parameter\n",
    "    # lambda_: is the regularization parameter of the denoising problem.\n",
    "    # max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "\n",
    "    # Write your code here.\n",
    "    l_t1 = time.time()\n",
    "    L = matrix_p2_norm(G(lamb=lambda_), tol=0)\n",
    "    l_t2 = time.time()\n",
    "    print(f\"Lipschitz Constant = {L}\")\n",
    "    x_updated = x0.copy()\n",
    "    f_vals = []\n",
    "    norm_vals = []\n",
    "    it_t1 = time.time()\n",
    "    for i in range(1, max_iterations+1):\n",
    "        current_grad = grad_f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec)\n",
    "        current_grad_norm = vector_norm(current_grad)\n",
    "        if current_grad_norm <= epsilon:\n",
    "            break\n",
    "        norm_vals.append(current_grad_norm)\n",
    "        f_vals.append(f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec))\n",
    "        x_updated = x_updated - (1.0 / L) * current_grad\n",
    "        f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "        grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "        print(f\"Step = {i}: Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "    it_t2 = time.time()\n",
    "    print(f\"Iterations (Total) time = {it_t2-it_t1}, Lip-Constant time = {l_t2-l_t1}\")\n",
    "    return x_updated, np.array(f_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:56:44.741804Z",
     "start_time": "2019-10-11T14:56:15.443589Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent.\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here.\n",
    "optimized_gd_lip, f_vals_lip = gradient_descent_lip(x0=noisy_image_vec, \n",
    "                                                    lambda_=lambda_, \n",
    "                                                    epsilon=epsilon, \n",
    "                                                    max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:58:37.236680Z",
     "start_time": "2019-10-11T14:58:30.545500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finding the optimal solution\n",
    "from scipy.sparse.linalg import spsolve\n",
    "x_star = spsolve(A=G(lamb=lambda_), b=noisy_image_vec)\n",
    "f_star = f(lamb=lambda_, x=csr_matrix(x_star).T, z_n=noisy_image_vec)\n",
    "denoised_image_star = x_star.reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_star, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T21:37:48.369104Z",
     "start_time": "2019-10-11T21:37:47.661286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the relative objective function vs number of iterations. \n",
    "rate_lip = (f_vals_lip - f_star) / (f_vals_lip[0] - f_star)\n",
    "\n",
    "denoised_image_gd_lip = optimized_gd_lip.toarray().reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_gd_lip, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max. Abs. Diff. between GD-Lip and Linear Solver = {np.abs(diff_lip).max()}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(rate_lip, label=(\"Gradient descent + Lip.\"), linewidth=5.0, color =\"black\")\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt.\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(rate_lip, label=(\"Gradient descent + Lip.\"), linewidth=5.0, color =\"black\")\n",
    "ax.set_yscale('log')\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt. (LOG)\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: is there a \"gap\" between the practical convergence rate and the theoretical convergence rate? Note that the denoising objective function is strongly convex. Marks: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:55:08.248267Z",
     "start_time": "2019-10-07T12:55:08.240827Z"
    }
   },
   "source": [
    "Yes. The practical convergence rate is significantly better than the theoritical convergence rate (i.e. the algorithm takes much less time than the theoritical bound), especially in the first few iterations (as seen on the log-scale curve)\n",
    "\n",
    "Since the denoising objective function is strongly convex, the theoritical iteration complexity can be proven to be $k \\geq \\frac{L}{\\mu} \\log(\\frac{f(x_0) - f^*}{\\epsilon})$.\n",
    "\n",
    "Substiuting the values of $L \\approx 33, \\mu \\approx 1, \\epsilon = 0.01$ (See question 7 to see how $\\mu$ is computed, and using the implemetation above to compute $f(x_0)$ and $f^*$, we can see that $k > \\thicksim850$ iterations! While in this experiment, the algorithm converges after $\\thicksim 350$ iterations.\n",
    "\n",
    "The reason is that, as mentioned in the lecture, these bounds on convergence rates are based on worst-case analysis, which might not necessiarily be the case in the denoising problem with this particular image. A more realistic estimate might be obtained by doing an average case analysis of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: implement gradient descent with line-search for the denoising problem. Marks: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T15:01:44.723230Z",
     "start_time": "2019-10-11T15:01:44.707751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a line-search-ls function here.  \n",
    "def line_search_ls(lambda_, x, f_x, grad_f_x):\n",
    "    # lambda_: the regularization parameter\n",
    "    # x: the current estimate of t=he variable\n",
    "    # f_x: the value of the objective function at x\n",
    "    # grad_f_x: the gradient of the objective function at x\n",
    "    \n",
    "    # Write your code here.\n",
    "    alpha = 1.0\n",
    "    while f(lamb=lambda_, x=x-alpha*grad_f_x, z_n=noisy_image_vec) >= f_x:\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "\n",
    "# Write gradient descent + line-search here.\n",
    "def gradient_descent_ls(x0, epsilon, lambda_, max_iterations):\n",
    "    # x0: is the initial guess for the x variables\n",
    "    # epsilon: is the termination tolerance parameter\n",
    "    # lambda_: is the regularization parameter of the denoising problem.\n",
    "    # max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "\n",
    "    # Write your code here.\n",
    "    x_updated = x0.copy()\n",
    "    f_vals = []\n",
    "    norm_vals = []\n",
    "    t1 = time.time()\n",
    "    for i in range(1, max_iterations+1):\n",
    "        current_grad = grad_f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec)\n",
    "        current_grad_norm = vector_norm(current_grad)\n",
    "        if current_grad_norm <= epsilon:\n",
    "            break\n",
    "        norm_vals.append(current_grad_norm)\n",
    "        f_vals.append(f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec))\n",
    "        alpha = line_search_ls(lambda_=lambda_, x=x_updated, f_x=f_vals[-1], grad_f_x=current_grad)\n",
    "        x_updated = x_updated - alpha * current_grad\n",
    "        f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "        grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "        print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "    t2 = time.time()\n",
    "    print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "    return x_updated, np.array(f_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with line-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T15:02:02.595252Z",
     "start_time": "2019-10-11T15:01:46.943086Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here.\n",
    "optimized_gd_ls, f_vals_ls = gradient_descent_ls(x0=noisy_image_vec, \n",
    "                                                 lambda_=lambda_, \n",
    "                                                 epsilon=epsilon, \n",
    "                                                 max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T21:38:16.991741Z",
     "start_time": "2019-10-11T21:38:16.290363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "\n",
    "# Plot the rellative objective function vs number of iterations. \n",
    "rate_ls = (f_vals_ls - f_star) / (f_vals_ls[0] - f_star)\n",
    "\n",
    "denoised_image_gd_ls = optimized_gd_ls.toarray().reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_gd_ls, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max. Abs. Diff. between GD-LS and Linear Solver = {np.abs(diff_ls).max()}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(rate_ls, label=(\"Gradient descent + LS\"), linewidth=5.0, color =\"black\")\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt.\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(rate_ls, label=(\"Gradient descent + LS\"), linewidth=5.0, color =\"black\")\n",
    "ax.set_yscale('log')\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt. (LOG)\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: What is the advantage of using line-search to compute the step-size at each iteration instead of using constant step-sizes equal to 1/L? Where L is the Lipschitz constant. Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time? Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time when you add computation of the Lipschitz constant in the running time? Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of number of required iterations? Marks: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question breakdown (Please note that all the experimental numbers are reported on my machine and might differ from any other machine, but the main conclusion should still hold):\n",
    "\n",
    "#### What is the advantage of using line-search to compute the step-size at each iteration instead of using constant step-sizes equal to 1/L? Where L is the Lipschitz constant. \n",
    "\n",
    "Computing the Lipschitz constant is a very expensive operation especially on large number of parameters. So, the advantage is to replace that expensive operation with a less costly one that still guarantees the decrease of the objective function at each iteration.\n",
    "\n",
    "#### Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time?\n",
    "\n",
    "Yes. Constant-step GD takes 198 seconds in total, where GD with line search takes 15.6 seconds in total. Both reaching the same tolernace as specified by the parameter epsilon.\n",
    "\n",
    "#### Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time when you add computation of the Lipschitz constant in the running time?\n",
    "\n",
    "Yes. Constant-step GD takes 26 seconds in total iterations time, and takes 172 seconds in computing the Lipschitz constant, where GD with line search takes 15.6 seconds in total. Both reaching the same tolernace as specified by the parameter epsilon.\n",
    "\n",
    "#### Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of number of required iterations?\n",
    "\n",
    "Yes. Constant-step GD takes 357 iterations, where GD with line search takes 91 iterations. Both reaching the same tolernace as specified by the parameter epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 5: implement gradient descent with Armijo line-search for the denoising problem. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T21:18:45.671120Z",
     "start_time": "2019-10-11T21:18:45.651829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a line-search-armijo function\n",
    "def line_search_arm(lambda_, x, f_x, grad_f_x, norm_grad_f_x, gamma):\n",
    "    # lambda_: the regularization parameter\n",
    "    # x: the current estimate of t=he variable\n",
    "    # f_x: the value of the objective function at x\n",
    "    # grad_f_x: the gradient of the objective function at x\n",
    "    # norm_grad_f_x: the norm of grad_f_x\n",
    "    # gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "\n",
    "    # Write your code here.\n",
    "    alpha = 1.0\n",
    "    loss = gamma * (norm_grad_f_x ** 2.0)\n",
    "    while f(lamb=lambda_, x=x-alpha*grad_f_x, z_n=noisy_image_vec) > f_x - alpha * loss:\n",
    "        alpha /= 2.0\n",
    "    return alpha\n",
    "    \n",
    "def gradient_descent_arm(x0, epsilon, lambda_, max_iterations, gamma):\n",
    "    # x0: is the initial guess for the x variables\n",
    "    # epsilon: is the termination tolerance parameter\n",
    "    # lambda_: is the regularization parameter of the denoising problem.\n",
    "    # max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "    # gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "    \n",
    "    # Write your code here.\n",
    "    x_updated = x0.copy()\n",
    "    f_vals = []\n",
    "    norm_vals = []\n",
    "    t1 = time.time()\n",
    "    for i in range(1, max_iterations+1):\n",
    "        current_grad = grad_f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec)\n",
    "        current_grad_norm = vector_norm(current_grad)\n",
    "        if current_grad_norm <= epsilon:\n",
    "            break\n",
    "        norm_vals.append(current_grad_norm)\n",
    "        f_vals.append(f(lamb=lambda_, x=x_updated, z_n=noisy_image_vec))\n",
    "        alpha = line_search_arm(lambda_=lambda_, x=x_updated, f_x=f_vals[-1], grad_f_x=current_grad, norm_grad_f_x=current_grad_norm, gamma=gamma)\n",
    "        x_updated = x_updated - alpha * current_grad\n",
    "        f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "        grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "        print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "    t2 = time.time()\n",
    "    print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "    return x_updated, np.array(f_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with Armijo line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T21:19:09.904425Z",
     "start_time": "2019-10-11T21:18:54.621323Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "gamma = 0.13\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here.\n",
    "optimized_gd_arm, f_vals_arm = gradient_descent_arm(x0=noisy_image_vec, \n",
    "                                                    lambda_=lambda_, \n",
    "                                                    epsilon=epsilon,\n",
    "                                                    gamma=gamma,\n",
    "                                                    max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T21:38:53.290401Z",
     "start_time": "2019-10-11T21:38:52.539045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the relative objective function vs number of iterations. \n",
    "\n",
    "rate_arm = (f_vals_arm - f_star) / (f_vals_arm[0] - f_star)\n",
    "\n",
    "denoised_image_gd_arm = optimized_gd_arm.toarray().reshape((m, n), order='F')\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(denoised_image_gd_arm, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max. Abs. Diff. between GD-ARM and Linear Solver = {np.abs(diff_arm).max()}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(rate_arm, label=(\"Gradient descent + ARM LS\"), linewidth=5.0, color =\"black\")\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt.\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(rate_arm, label=(\"Gradient descent + ARM LS\"), linewidth=5.0, color =\"black\")\n",
    "ax.set_yscale('log')\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Relative distance to opt. (LOG)\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of running time? Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of number of required iterations? Explain any performance differences between the two approaches. Marks: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question breakdown (Please note that all the experimental numbers are reported on my machine and might differ from any other machine, but the main conclusion should still hold):\n",
    "\n",
    "#### Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of running time?\n",
    "Yes. GD with simple line search takes 15.6 seconds in total, where GD with Armijo line search with $\\gamma = 0.13$ takes 14 seconds in total. Both reaching the same tolernace as specified by the parameter epsilon.\n",
    "\n",
    "#### Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of number of required iterations? \n",
    "\n",
    "Yes. GD with simple line search takes 91 iterations, where GD with Armijo line search with $\\gamma = 0.13$ takes 77 iterations in total. Both reaching the same tolernace as specified by the parameter epsilon. \n",
    "\n",
    "#### Explain any performance differences between the two approaches.\n",
    "\n",
    "The main difference between the two methods is that the Armijo line search is more \"agressive\" in finding the $\\alpha$ parameter than the simple line search. The check is done such that the objective function decreased sufficiently (by at least $\\alpha \\gamma ||\\nabla f(x)||_2^2$), not just decreased as in simple line search. This may lead to more line search iterations (and hence a longer time per gradient descent step), but it will also lead to less total number of gradient descent steps, since the convex function is forced to be decreased more every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: prove that the denoising objective function is strongly convex. What is its strong convexity parameter? Marks: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From assignment 1 we know that, for the denoising objective function:\n",
    "$$ \n",
    "\\nabla^2 f(x) = \\lambda_{reg} (D^*.D) + I\n",
    "$$\n",
    "\n",
    "Where $\\lambda_{reg}$ is the regularization parameter.\n",
    "\n",
    "We know that the defintion of a strongly convex (twice-differentiable) function is:\n",
    "$$\n",
    "y^T.\\nabla^2 f(x).y \\geq \\mu ||y||^2_2\n",
    "$$\n",
    "\n",
    "For now, let's call $G = \\nabla^2 f(x) = \\lambda_{reg} (D^*.D) + I$. Which is a constant that does not depend on $x$.\n",
    "\n",
    "We then rewrite the strong convexity condition as: $y^T.G.y \\geq \\mu ||y||^2_2$\n",
    "\n",
    "Starting with left-hand side, we replace the product $G.y$ with $\\lambda y$, where $\\lambda$ is an eigenvalue of the matrix $G$. And noticing that $y^T.y = ||y||_2^2$. We then have:\n",
    "\n",
    "$$\n",
    "y^T.G.y =\n",
    "y^T.\\lambda.y =\n",
    "\\lambda.(y^T.y) =\n",
    "\\lambda ||y||_2^2 \\geq \\mu ||y||^2_2\n",
    "$$\n",
    "\n",
    "\n",
    "So, the denoising objective function is strongly convex, and the strong convexity parameter is given by the smallest positive eigenvalue of the hessian (Which will always exist given that $G$ is positive semi-definite, so all eigenvalues are greater than or equal to zero).\n",
    "\n",
    "Using the function $eigsh$ with $\\lambda = 4.0$ to compute the strong convexity parameter, we find that $\\mu = \\lambda_{min} \\approx 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 8: Prove that Armijo line-search will terminate after a finite number of steps. Hint: show that there exists a step-size $$\\alpha^*>0$$ such that for any step-size smaller than $$\\alpha^*$$ the termination condition of Armijo line-search is satisfied. How many iterations will be required in worst-case for Armijo line-search to terminate? Marks 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploaded to Learn separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: what is the running time for gradient descent with Armijo line-search for the denoising problem to achieve $$f(x_k) - f^* \\le \\epsilon$$ ?. The running time is computed by multiplying the worst-case iteration complexity times the FLOPS at each iteration. The FLOPS at each iteration is the number of additions, subtractions, multiplications and divisions that are performed during the current iteration. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T22:38:08.428778Z",
     "start_time": "2019-10-11T22:38:08.422548Z"
    }
   },
   "source": [
    "Uploaded to Learn separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 10: prove the convergence rate and iteration complexity for gradient descent with constant step-sizes (equal to 1/L) for strongly convex functions. Marks: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploaded to Learn separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
