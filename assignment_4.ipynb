{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 4\n",
    "\n",
    "# This is a mini-project assignment that includes only programming questions. You are asked to implement optimization algorithms for ML classification problems. \n",
    "\n",
    "## Marking of this assignment will be based on the correctness of your ML pipeline and efficiency of your code. \n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and to Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:06:20.733776Z",
     "start_time": "2019-11-15T20:06:20.435718Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, sys, scikit-image, skimage, matplotlib\n",
    "\n",
    "import time\n",
    "import math \n",
    "import random \n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested way of loading data to python for the assigment. There are alternatives of course, you can use your preferred way if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:06:24.046041Z",
     "start_time": "2019-11-15T20:06:23.717558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the LIBSVM package from here: https://www.csie.ntu.edu.tw/~cjlin/libsvm/#download \n",
    "# If your download is successfull you should have the folder with name: libsvm-3.24.\n",
    "# We will use this package to load datasets. \n",
    "\n",
    "# Enter the downloaded folder libsvm-3.24 through your terminal. \n",
    "# Run make command to compile the package.\n",
    "\n",
    "# Load this auxiliary package.\n",
    "import sys\n",
    "\n",
    "# add here your path to the folder libsvm-3.24/python\n",
    "path = \"/home/oymamatt/workplace/opt4ml/optmization4ML/libsvm-3.24/python/\"\n",
    "# Add the path to the Python paths so Python can find the module.\n",
    "sys.path.append(path)\n",
    "\n",
    "# Load the LIBSVM module.\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets that you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T17:03:44.886910Z",
     "start_time": "2019-11-11T17:03:44.884884Z"
    }
   },
   "outputs": [],
   "source": [
    "# There is an extended selection of classification and regression datasets \n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
    "\n",
    "# Out of all these datasets you will need the following 3 datasets, which are datasets for classification problems.\n",
    "# \n",
    "# a9a dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a \n",
    "# This dataset is small, it is recommened to start your experiments with this dataset.\n",
    "#\n",
    "# news20.binary dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#news20.binary\n",
    "#\n",
    "# covtype.binary dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#covtype.binary\n",
    "#\n",
    "# Exploit the sparsity of the problem when you implement optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:55:28.486649Z",
     "start_time": "2019-11-15T20:55:28.476132Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_sparse(list_dict_features, list_labels=None):\n",
    "    samples = len(list_dict_features)\n",
    "    \n",
    "    # protect against an all zero feature\n",
    "    feats = {\n",
    "        f_id: idx \n",
    "        for idx, f_id in \n",
    "        enumerate(sorted(list({s for sample in list_dict_features for s in sample.keys()})))\n",
    "    }\n",
    "    \n",
    "    num_of_features = len(feats)\n",
    "    \n",
    "    mat = sp.dok_matrix((samples, num_of_features), dtype=np.float64)\n",
    "    \n",
    "    for sample_id, sample in enumerate(list_dict_features):\n",
    "        for feature_id, feature_val in sample.items():\n",
    "            mat[sample_id, feats[feature_id]] = feature_val\n",
    "\n",
    "    mat = mat.tocsr()\n",
    "    l = csr_matrix(list_labels).transpose() if list_labels else None\n",
    "    return mat, l\n",
    "\n",
    "\n",
    "def split_train_validate(A, B, valid_ratio=0.1, seed=777):\n",
    "    samples = B.shape[0]\n",
    "    idx = np.arange(samples)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "    valid_size = math.floor(valid_ratio * samples)\n",
    "    train_size = samples - valid_size\n",
    "    return A[idx[:train_size], :], B[idx[:train_size]], A[idx[train_size:], :], B[idx[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T04:28:07.127035Z",
     "start_time": "2019-11-18T04:28:06.830490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add here your path to the dataset file\n",
    "path = \"/home/oymamatt/workplace/opt4ml/optmization4ML/a9a.txt\"\n",
    "# path = \"/home/oymamatt/workplace/opt4ml/optmization4ML/iris\"\n",
    "# path = \"/home/oymamatt/workplace/opt4ml/optmization4ML/news20.binary\"\n",
    "# Use \"svm_read_problem\" function to load data for your assignment.\n",
    "# it will store the labels in \"b\" and the data matrix in \"A\".\n",
    "B, A = svm_read_problem(path)\n",
    "\n",
    "# Note that matrix \"A\" stores the data in a sparse format. \n",
    "# In particular matrix \"A\" is a list of dictionaries. \n",
    "# The length of the list gives you the number of samples.\n",
    "# Each entry in the list is a dictionary. The keys of the dictionary are the non-zero features.\n",
    "# The values of the dictionary for each key is a list which gives you the feature value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:58:07.425368Z",
     "start_time": "2019-11-12T01:58:07.423411Z"
    }
   },
   "outputs": [],
   "source": [
    "# All datasets above consist of training and testing data. \n",
    "\n",
    "# You should seperate the training data into training and validation data.\n",
    "# Follow the instructions from the lectures about how you can use both training and validation data.\n",
    "# You can use 10% of the training data as validation data and the remaining 90% to train the models.\n",
    "# This is a suggested percentage, you can do otherwise if you wish.\n",
    "\n",
    "# Do not use the testing data to influence training in any way. Do not use the testing data at all.\n",
    "# Only your instructor and TA will use the testing data to measure generalization error. \n",
    "# If you do use the testing data to tune parameters or for training of the algorithms we will figure it out :-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to solve the following optimization problems \n",
    "\n",
    "Hinge-loss\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d, \\beta \\in \\mathbb{R}} \\ \\frac{1}{n} \\sum_{i=1}^n \\max \\{0,1-b_i(a_i^Tx + \\beta)\\},$$\n",
    "where $a_i\\in\\mathbb{R}^d$ is the feature vector for sample $i$ and $b_i$ is the label of sample $i$. The sub-gradient of the hinge-loss is given in the lecture slides (note that there is a small difference due to the intercept $\\beta$). A smooth approximation of the function $f(z):=\\max\\{0,1-z\\}$ is given by\n",
    "$$\n",
    "\\psi_\\mu(z) = \n",
    "\\begin{cases}\n",
    "0 & z\\ge 1\\\\\n",
    "(1-z)^2 & \\mu < z < 1 \\\\\n",
    "(1-\\mu)^2 + 2(1-\\mu)(\\mu-z) & z \\le \\mu.\n",
    "\\end{cases}\n",
    "$$\n",
    "You can use the smooth approximation $\\psi_\\mu(z)$ for methods that work only for smooth functions. For sub-gradient methods you should use the sub-gradient.\n",
    "\n",
    "L2-regularized logistic regression\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d,\\beta\\in\\mathbb{R}} \\ \\lambda \\|x\\|_2^2 + \\frac{1}{n} \\sum_{i=1}^n \\log (1+ \\exp(-b_i(a_i^Tx + \\beta))).$$\n",
    "This is a smooth objective function, therefore, you should use gradient methods to solve it. You do not need sub-gradient methods for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T04:28:30.725514Z",
     "start_time": "2019-11-18T04:28:30.677419Z"
    }
   },
   "outputs": [],
   "source": [
    "def hinge_margin(b, a, x, beta):\n",
    "    m = a.dot(x) + beta\n",
    "    m = b.multiply(m)\n",
    "    return m.toarray()\n",
    "\n",
    "def hinge_predict(a, x, beta):\n",
    "    m = a.dot(x) + beta\n",
    "    labels = np.sign(m)\n",
    "    return labels\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "def hinge_loss(b, a, x, beta):\n",
    "    margin = hinge_margin(b, a, x, beta)\n",
    "    margin_max = 1.0 - margin\n",
    "    margin_max[margin_max < 0.0] = 0.0\n",
    "    loss = np.sum(margin_max) / b.shape[0]\n",
    "    return loss\n",
    "\n",
    "def hinge_grad(b, a, x, beta, reduce=True):\n",
    "    margin = hinge_margin(b, a, x, beta)\n",
    "    margin_mask = (margin >= 1.0).squeeze()\n",
    "    \n",
    "    acc_x = -1.0 * b.multiply(a)\n",
    "    acc_x[margin_mask, :] = acc_x[margin_mask, :].multiply(0.0)\n",
    "    if reduce:\n",
    "        g_x = acc_x.sum(axis=0).T / b.shape[0]\n",
    "    else:\n",
    "        g_x = acc_x\n",
    "    \n",
    "    acc_beta = -1.0 * b\n",
    "    acc_beta[margin_mask] = 0.0\n",
    "    \n",
    "    if reduce:\n",
    "        g_beta = acc_beta.sum() / b.shape[0]\n",
    "    else:\n",
    "        g_beta = acc_beta\n",
    "    \n",
    "    return g_x, g_beta\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "def smooth_hinge_loss(b, a, x, beta, mu):\n",
    "    z = hinge_margin(b, a, x, beta)\n",
    "    case_1_mask = (z >= 1.0).squeeze(axis=1)\n",
    "    case_2_mask = ((mu < z) & (z < 1.0)).squeeze(axis=1)\n",
    "    case_3_mask = (z <= mu).squeeze(axis=1)\n",
    "    \n",
    "    f = z.copy()\n",
    "    f[case_1_mask] = 0.0\n",
    "    f[case_2_mask] = (1.0 - z[case_2_mask]) ** 2.0\n",
    "    f[case_3_mask] = (1.0 - mu) ** 2.0 + 2.0 * (1.0 - mu) * (mu - z[case_3_mask])\n",
    "\n",
    "    loss = np.sum(f) / b.shape[0]\n",
    "    return loss\n",
    "\n",
    "def smooth_hinge_grad(b, a, x, beta, mu, reduce=True):\n",
    "    z = hinge_margin(b, a, x, beta)\n",
    "    \n",
    "    case_1_mask = (z >= 1.0).squeeze(axis=1)\n",
    "    case_2_mask = ((mu < z) & (z < 1.0)).squeeze(axis=1)\n",
    "    case_3_mask = (z <= mu).squeeze(axis=1)\n",
    "    \n",
    "    acc_x = b.multiply(a)\n",
    "    acc_x[case_1_mask, :] *= 0.0\n",
    "    acc_x[case_2_mask, :] = acc_x[case_2_mask, :].multiply(2.0 * (1.0 - z[case_2_mask, :]))\n",
    "    acc_x[case_3_mask, :] *= -2.0 * (1.0 - mu)\n",
    "    if reduce:\n",
    "        g_x = acc_x.sum(axis=0).T / b.shape[0]\n",
    "    else:\n",
    "        g_x = acc_x\n",
    "    \n",
    "    acc_beta = 1.0 * b\n",
    "    acc_beta[case_1_mask, :] *= 0.0\n",
    "    acc_beta[case_2_mask, :] = acc_beta[case_2_mask, :].multiply(2.0 * (1.0 - z[case_2_mask, :]))\n",
    "    acc_beta[case_3_mask, :] *= -2.0 * (1.0 - mu)\n",
    "    if reduce:\n",
    "        g_beta = acc_beta.sum() / b.shape[0]\n",
    "    else:\n",
    "        g_beta = acc_beta\n",
    "    \n",
    "    return g_x, g_beta\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "def reg_logistic_loss(b, a, x, beta, lambda_):\n",
    "    margin = hinge_margin(b, a, x, beta)\n",
    "    logistic = np.log(1.0 + np.exp(-1.0 * margin))\n",
    "    reg = lambda_ * (np.linalg.norm(x) ** 2.0)\n",
    "    loss = reg + (np.sum(logistic) / b.shape[0])\n",
    "    return loss\n",
    "\n",
    "def reg_logistic_grad(b, a, x, beta, lambda_, reduce=True):\n",
    "    margin = hinge_margin(b, a, x, beta)\n",
    "    z = -1.0 * margin\n",
    "    exp_z = np.exp(z)\n",
    "    log_grad = exp_z / (1.0 + exp_z)\n",
    "    \n",
    "    z_x = -1.0 * b.multiply(a)\n",
    "    acc_x = z_x.multiply(log_grad)\n",
    "    if reduce:\n",
    "        g_x = 2.0 * lambda_ * x + acc_x.sum(axis=0).T / b.shape[0]\n",
    "    else:\n",
    "        g_x = 2.0 * lambda_ * x.T + acc_x\n",
    "    \n",
    "    z_beta = -1.0 * b\n",
    "    acc_beta = z_beta.multiply(log_grad)\n",
    "    if reduce:\n",
    "        g_beta = acc_beta.sum() / b.shape[0]\n",
    "    else:\n",
    "        g_beta = acc_beta.todense()\n",
    "    \n",
    "    return g_x, g_beta\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "def reg_logistic_predict(a, x, beta):\n",
    "    m = a.dot(x) + beta\n",
    "    probs = 1.0 / (1.0 + np.exp(-1.0 * m))\n",
    "    labels = probs.copy()\n",
    "    labels[probs > 0.5] = 1.0\n",
    "    labels [probs <= 0.5] = -1.0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:07:13.106655Z",
     "start_time": "2019-11-15T20:07:13.097219Z"
    }
   },
   "outputs": [],
   "source": [
    "class HingeLossModel:\n",
    "    def loss(self, b, a, x, beta):\n",
    "        return hinge_loss(b=b, a=a, x=x, beta=beta)\n",
    "    \n",
    "    def grad(self, b, a, x, beta, reduce=True):\n",
    "        return hinge_grad(b=b, a=a, x=x, beta=beta, reduce=reduce)\n",
    "    \n",
    "    def predict(self, a, x, beta):\n",
    "        return hinge_predict(a=a, x=x, beta=beta)\n",
    "\n",
    "\n",
    "class SmoothHingeLossModel:\n",
    "    def __init__(self, mu=0.1):\n",
    "        self.mu = mu\n",
    "    \n",
    "    def loss(self, b, a, x, beta):\n",
    "        return smooth_hinge_loss(b=b, a=a, x=x, beta=beta, mu=self.mu)\n",
    "    \n",
    "    def grad(self, b, a, x, beta, reduce=True):\n",
    "        return smooth_hinge_grad(b=b, a=a, x=x, beta=beta, mu=self.mu, reduce=reduce)\n",
    "    \n",
    "    def predict(self, a, x, beta):\n",
    "        return hinge_predict(a=a, x=x, beta=beta)\n",
    "\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, lambda_=0.01):\n",
    "        self._lambda = lambda_\n",
    "        \n",
    "    def loss(self, b, a, x, beta):\n",
    "        return reg_logistic_loss(b=b, a=a, x=x, beta=beta, lambda_=self._lambda)\n",
    "    \n",
    "    def grad(self, b, a, x, beta, reduce=True):\n",
    "        return reg_logistic_grad(b=b, a=a, x=x, beta=beta, lambda_=self._lambda, reduce=reduce)\n",
    "    \n",
    "    def predict(self, a, x, beta):\n",
    "        return reg_logistic_predict(a=a, x=x, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T04:29:37.996437Z",
     "start_time": "2019-11-18T04:29:37.909239Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this assignment you will need the following methods\n",
    "\n",
    "\n",
    "# 1) Stochastic sub-gradient\n",
    "# 2) Stochastic gradient\n",
    "# 3) Mini-batch (sub-)gradient (you will have to decide what batching strategy to use, see lecture slides)\n",
    "class Optimizer123:\n",
    "    def __init__(self, model, init_lr=0.01, grad_all_every=10, decrease_lr=False, batch_size=1, epsilon=0.000001, max_iterations=200, seed=777):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        self.seed = seed\n",
    "        self.init_lr = init_lr\n",
    "        self.grad_all_every = grad_all_every\n",
    "        self.decrease_lr = decrease_lr\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.steps = None\n",
    "        \n",
    "    def fit(self, a_t, b_t):\n",
    "        np.random.seed(self.seed)\n",
    "        self.x = csr_matrix(np.random.rand(a_t.shape[1], 1))\n",
    "        self.beta = 0.0\n",
    "        self.sgd(a_t_all=a_t, b_t_all=b_t)\n",
    "\n",
    "    def predict(self, a):\n",
    "        predictions = self.model.predict(a=a, x=self.x, beta=self.beta)\n",
    "        return list(itertools.chain.from_iterable(predictions.tolist()))\n",
    "    \n",
    "    def sgd(self, a_t_all, b_t_all):\n",
    "        x_updated = self.x.copy().toarray()\n",
    "        beta_updated = self.beta\n",
    "        f_vals = []\n",
    "        norm_vals = []\n",
    "        samples = b_t_all.shape[0]\n",
    "        t1 = time.time()\n",
    "        for i in range(1, self.max_iterations+1):\n",
    "            if i % self.grad_all_every == 0:\n",
    "                a_t = a_t_all\n",
    "                b_t = b_t_all\n",
    "            else:\n",
    "                batch_idx = np.random.randint(low=0, high=samples, size=self.batch_size)\n",
    "                a_t = a_t_all[batch_idx, :]\n",
    "                b_t = b_t_all[batch_idx, :]\n",
    "            \n",
    "            current_grad_x, current_grad_beta = self.model.grad(b=b_t, a=a_t, x=x_updated, beta=beta_updated)\n",
    "            total_grad = np.vstack((current_grad_x, [current_grad_beta]))\n",
    "            current_grad_norm = np.linalg.norm(total_grad)\n",
    "            norm_vals.append(current_grad_norm)\n",
    "            f_vals.append(self.model.loss(b=b_t, a=a_t, x=x_updated, beta=beta_updated))\n",
    "            \n",
    "            alpha = self.init_lr / (float(i) if self.decrease_lr else 1.0)\n",
    "            x_updated = x_updated - alpha * current_grad_x\n",
    "            beta_updated = beta_updated - alpha * current_grad_beta\n",
    "            f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "            grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "            \n",
    "            step_num = f\"--{i}--\" if i % self.grad_all_every == 0 else str(i)\n",
    "            print(f\"Step = {step_num}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "        self.x = x_updated\n",
    "        self.beta = beta_updated\n",
    "        self.steps = np.array(f_vals)  \n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "# 4) Stochastic average sub-gradient (SAG)\n",
    "# 5) Stochastic average gradient (SAG)\n",
    "class Optimizer45:\n",
    "    def __init__(self, model, init_lr=0.01, decrease_lr=False, epsilon=0.000001, max_iterations=400, seed=777):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        self.seed = seed\n",
    "        self.init_lr = init_lr\n",
    "        self.decrease_lr = decrease_lr\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.steps = None\n",
    "        \n",
    "    def fit(self, a_t, b_t):\n",
    "        np.random.seed(self.seed)\n",
    "        self.x = csr_matrix(np.random.rand(a_t.shape[1], 1))\n",
    "        self.beta = 0.0\n",
    "        self.sag(a_t_all=a_t, b_t_all=b_t)\n",
    "\n",
    "    def predict(self, a):\n",
    "        predictions = self.model.predict(a=a, x=self.x, beta=self.beta)\n",
    "        return list(itertools.chain.from_iterable(predictions.tolist()))\n",
    "    \n",
    "    def sag(self, a_t_all, b_t_all):\n",
    "        x_updated = self.x.copy().toarray()\n",
    "        beta_updated = self.beta\n",
    "        f_vals = []\n",
    "        norm_vals = []\n",
    "        samples = b_t_all.shape[0]\n",
    "        t1 = time.time()\n",
    "        \n",
    "        all_grad_x, all_grad_beta = self.model.grad(b=b_t_all, a=a_t_all, \n",
    "                                                    x=x_updated, beta=beta_updated, \n",
    "                                                    reduce=False)\n",
    "        \n",
    "        grad_x_sum = all_grad_x.sum(axis=0).T\n",
    "        grad_beta_sum = float(all_grad_beta.sum(axis=0).squeeze())\n",
    "        \n",
    "        for i in range(1, self.max_iterations+1):\n",
    "            sample_idx = int(np.random.randint(low=0, high=samples, size=1).squeeze())\n",
    "            \n",
    "            a_t = a_t_all[sample_idx, :]\n",
    "            b_t = b_t_all[sample_idx, :]\n",
    "            \n",
    "            current_grad_x, current_grad_beta = self.model.grad(b=b_t, a=a_t, x=x_updated, beta=beta_updated)\n",
    "            old_grad_x, old_grad_beta = all_grad_x[sample_idx, :].T, float(all_grad_beta[sample_idx, :])\n",
    "            all_grad_x[sample_idx, :] = current_grad_x.T\n",
    "            all_grad_beta[sample_idx, :] = float(current_grad_beta)\n",
    "            \n",
    "            grad_x_sum = current_grad_x - old_grad_x + grad_x_sum\n",
    "            grad_beta_sum = float(current_grad_beta) - old_grad_beta + grad_beta_sum\n",
    "            \n",
    "            total_grad = np.vstack((grad_x_sum, [grad_beta_sum])) / samples\n",
    "            current_grad_norm = np.linalg.norm(total_grad)\n",
    "            norm_vals.append(current_grad_norm)\n",
    "            f_vals.append(self.model.loss(b=b_t, a=a_t, x=x_updated, beta=beta_updated))\n",
    "\n",
    "            if current_grad_norm <= self.epsilon:\n",
    "                break\n",
    "            \n",
    "            alpha = self.init_lr / (float(i) if self.decrease_lr else 1.0)\n",
    "            x_updated = x_updated - (alpha / samples) * grad_x_sum\n",
    "            beta_updated = beta_updated - (alpha / samples) * grad_beta_sum\n",
    "            f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "            grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "            print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "        self.x = x_updated\n",
    "        self.beta = beta_updated\n",
    "        self.steps = np.array(f_vals) \n",
    "    \n",
    "#=============================================================================\n",
    "\n",
    "# 6) Gradient descent with Armijo line-search\n",
    "class Optimizer6:\n",
    "    def __init__(self, model, epsilon=0.000001, max_iterations=400, gamma=0.1, seed=777):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        self.gamma = gamma\n",
    "        self.seed = seed\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.steps = None\n",
    "        \n",
    "    def fit(self, a_t, b_t):\n",
    "        np.random.seed(self.seed)\n",
    "        self.x = csr_matrix(np.random.rand(a_t.shape[1], 1))\n",
    "        self.beta = 0.0\n",
    "        self.gradient_descent_arm(a_t=a_t, b_t=b_t)\n",
    "\n",
    "    def predict(self, a):\n",
    "        predictions = self.model.predict(a=a, x=self.x, beta=self.beta)\n",
    "        return list(itertools.chain.from_iterable(predictions.tolist()))\n",
    "    \n",
    "    def line_search_arm(self, a, b, x, beta, f, grad_x, grad_beta, current_grad_norm):\n",
    "        decrease = self.gamma * (current_grad_norm ** 2.0)\n",
    "        alpha = 1.0\n",
    "        while self.model.loss(b=b, a=a, x=x-alpha*grad_x, beta=beta-alpha*grad_beta) > f - alpha * decrease:\n",
    "            alpha /= 2.0\n",
    "        return alpha\n",
    "    \n",
    "    def gradient_descent_arm(self, a_t, b_t):\n",
    "        x_updated = self.x.copy().toarray()\n",
    "        beta_updated = self.beta\n",
    "        f_vals = []\n",
    "        norm_vals = []\n",
    "        t1 = time.time()\n",
    "        for i in range(1, self.max_iterations+1):\n",
    "            current_grad_x, current_grad_beta = self.model.grad(b=b_t, a=a_t, x=x_updated, beta=beta_updated)\n",
    "            \n",
    "            total_grad = np.vstack((current_grad_x, [current_grad_beta]))\n",
    "            current_grad_norm = np.linalg.norm(total_grad)\n",
    "            norm_vals.append(current_grad_norm)\n",
    "            f_vals.append(self.model.loss(b=b_t, a=a_t, x=x_updated, beta=beta_updated))\n",
    "            \n",
    "            if current_grad_norm <= self.epsilon:\n",
    "                break\n",
    "            \n",
    "            alpha = self.line_search_arm(a=a_t, b=b_t, f=f_vals[-1],\n",
    "                                         x=x_updated, beta=beta_updated,\n",
    "                                         grad_x=current_grad_x, \n",
    "                                         grad_beta=current_grad_beta, \n",
    "                                         current_grad_norm=current_grad_norm)\n",
    "            \n",
    "            x_updated = x_updated - alpha * current_grad_x\n",
    "            beta_updated = beta_updated - alpha * current_grad_beta\n",
    "            \n",
    "            f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "            grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "            print(f\"Step = {i}: alpha = {alpha}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad. Diff. = {grad_diff}\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "        self.x = x_updated\n",
    "        self.beta = beta_updated\n",
    "        self.steps = np.array(f_vals)\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "# 7) Acceleratd gradient with Armijo line-search (the same method as Q5 in Assignemnt 3)\n",
    "class Optimizer7:\n",
    "    def __init__(self, model, epsilon=0.000001, max_iterations=400, gamma=0.1, seed=777):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        self.gamma = gamma\n",
    "        self.seed = seed\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "        self.steps = None\n",
    "        \n",
    "    def fit(self, a_t, b_t):\n",
    "        np.random.seed(self.seed)\n",
    "        self.x = csr_matrix(np.random.rand(a_t.shape[1], 1))\n",
    "        self.beta = 0.0\n",
    "        self.accelerated_gd_practical(a_t=a_t, b_t=b_t)\n",
    "\n",
    "    def predict(self, a):\n",
    "        predictions = self.model.predict(a=a, x=self.x, beta=self.beta)\n",
    "        return list(itertools.chain.from_iterable(predictions.tolist()))\n",
    "\n",
    "    def line_search_arm_prac(self, a, b, x, beta, f, grad_x, grad_beta, current_grad_norm):\n",
    "        decrease = self.gamma * (current_grad_norm ** 2.0)\n",
    "        alpha = 1.0\n",
    "        while self.model.loss(b=b, a=a, x=x-alpha*grad_x, beta=beta-alpha*grad_beta) > f - alpha * decrease:\n",
    "            alpha /= 2.0\n",
    "        return alpha\n",
    "\n",
    "    def accelerated_gd_practical(self, a_t, b_t):\n",
    "        y = self.x.copy().toarray()\n",
    "        beta_y = self.beta\n",
    "\n",
    "        t = 1.0\n",
    "\n",
    "        x_updated = self.x.copy().toarray()\n",
    "        beta_updated = self.beta\n",
    "\n",
    "        f_vals = []\n",
    "        norm_vals = []\n",
    "        t1 = time.time()\n",
    "        for i in range(1, self.max_iterations+1):\n",
    "            current_grad_x, current_grad_beta = self.model.grad(b=b_t, a=a_t, x=y, beta=beta_y)\n",
    "            \n",
    "            total_grad = np.vstack((current_grad_x, [current_grad_beta]))\n",
    "            current_grad_norm = np.linalg.norm(total_grad)\n",
    "            norm_vals.append(current_grad_norm)\n",
    "            f_vals.append(self.model.loss(b=b_t, a=a_t, x=x_updated, beta=beta_updated))\n",
    "            \n",
    "            if current_grad_norm <= self.epsilon:\n",
    "                break\n",
    "\n",
    "            fy = self.model.loss(b=b_t, a=a_t, x=y, beta=beta_y)\n",
    "            \n",
    "            alpha = self.line_search_arm_prac(a=a_t, b=b_t, f=fy,\n",
    "                                              x=y, beta=beta_y,\n",
    "                                              grad_x=current_grad_x, \n",
    "                                              grad_beta=current_grad_beta, \n",
    "                                              current_grad_norm=current_grad_norm)\n",
    "            \n",
    "            x_updated_new = y - alpha * current_grad_x\n",
    "            beta_updated_new = beta_y - alpha * current_grad_beta\n",
    "            t_new = (1.0 + np.sqrt(1 + 4 * t ** 2)) / 2.0\n",
    "            y = x_updated_new + ((t - 1.0) / t_new) * (x_updated_new - x_updated)\n",
    "            beta_y = beta_updated_new + ((t - 1.0) / t_new) * (beta_updated_new - beta_updated)\n",
    "\n",
    "            t = t_new\n",
    "            x_updated = x_updated_new\n",
    "            beta_updated = beta_updated_new\n",
    "\n",
    "            f_diff = (f_vals[-1] - f_vals[-2]) if len(f_vals) > 1 else None\n",
    "            grad_diff = (norm_vals[-1] - norm_vals[-2]) if len(norm_vals) > 1 else None\n",
    "            print(f\"Step = {i}: alpha = {alpha}, t = {t}, Function = {f_vals[-1]}, Function Diff. =  {f_diff}, Grad. Norm = {norm_vals[-1]}, Grad.Norm. Diff. = {grad_diff}\")\n",
    "        t2 = time.time()\n",
    "        print(f\"Iterations (Total) time = {t2-t1}\")\n",
    "        self.x = x_updated\n",
    "        self.beta = beta_updated\n",
    "        self.steps = np.array(f_vals)\n",
    "        \n",
    "# Information is provided in the lecture slides about parameter tuning and termination.\n",
    "# However, the final decision of any parameter tuning and termination criteria is up to the students to make. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation error: measure the validation error by calculating\n",
    "$$\n",
    "\\frac{1}{t}\\sum_{i\\in\\mbox{validation data}} \\left| \\ b_i^{\\mbox{your model}} - b_i^{\\mbox{true}} \\ \\right|\n",
    "$$\n",
    "where $t$ is the number of samples in your validation set. $b_i^{\\mbox{true}}$ is the true label of the $i$-th sample. $b_i^{\\mbox{your model}}$ is the label of the $i$-th sample of your model.\n",
    "\n",
    "For hinge loss calculate $$b_i^{\\mbox{your model}}:= \\mbox{sign}(a_i^Tx + \\beta).$$\n",
    "\n",
    "For logistic regression calculate the predicted label by\n",
    "$$\n",
    "b_i^{\\mbox{your model}}=\n",
    "\\begin{cases}\n",
    "1 & \\mbox{if } \\frac{1}{1+e^{-(a_i^Tx + \\beta)}} > 0.5\\\\\n",
    "-1 & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:07:39.097008Z",
     "start_time": "2019-11-15T20:07:39.093844Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation_error(preds, trues):\n",
    "    arr_preds = np.array(preds)\n",
    "    arr_trues = np.array(trues)\n",
    "    assert arr_preds.shape == arr_trues.shape\n",
    "    return np.abs(arr_preds - arr_trues).sum() / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Use the ML pipeline that is mentioned in slide 60 of Lecture 11 to train your model for the logistic regression problem (the hinge-loss problem does not have any hyper-parameters). Pick any algorithm that you want from the above suggested list to train the models. Report your ML pipeline. Print your Generalization Error. We will not measure running time for this pipeline. Running time will be measure only in Q2. Marks: 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:43:58.139585Z",
     "start_time": "2019-11-15T20:41:25.930682Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "A_s, b_s = to_sparse(A, B)\n",
    "a_t, b_t, a_v, b_v = split_train_validate(A_s, b_s)\n",
    "\n",
    "# Train/eval loop\n",
    "eval_preds = []\n",
    "for l in [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
    "    for g in [0.1, 0.15, 0.2, 0.3]:\n",
    "        print(f\"lambda={l}, gamma={g}\")\n",
    "        optm = Optimizer7(model=LogisticRegressionModel(lambda_=l), gamma=g, epsilon=0.001, max_iterations=200)\n",
    "        optm.fit(a_t=a_t, a_t=b_t)\n",
    "        preds = optm.predict(a_v)\n",
    "        valid_error = validation_error(preds, b_v.toarray().T.tolist()[0])\n",
    "        eval_preds.append((l, g, valid_error))\n",
    "        print(\"-=\" * 40)\n",
    "        print(\"-=\" * 40)\n",
    "\n",
    "# best lambda: 0.01, gamma: 0.1\n",
    "for t in eval_preds:\n",
    "    print(f\"===> lambda={t[0]}, gamma={t[1]}, validation error={t[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T20:55:50.500785Z",
     "start_time": "2019-11-15T20:55:50.491280Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMethod:\n",
    "    def __init__(self):\n",
    "        self.optimizer = Optimizer123(model=HingeLossModel(), \n",
    "                                      init_lr=0.01, \n",
    "                                      decrease_lr=False, \n",
    "                                      batch_size=10000, \n",
    "                                      epsilon=0.000001, \n",
    "                                      max_iterations=5700, \n",
    "                                      seed=777)\n",
    "        self.x = None\n",
    "        self.beta = None\n",
    "    \n",
    "    def fit(self, train_data, train_label):\n",
    "        a_t, b_t = to_sparse(train_data, train_label)\n",
    "        self.optimizer.fit(a_t=a_t, b_t=b_t)\n",
    "        self.x = self.optimizer.x\n",
    "        self.beta = self.optimizer.beta\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        a, _ = to_sparse(test_data)\n",
    "        return self.optimizer.predict(a=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Plot the objective function (y-axis) vs running time in sec (x-axis). Have one plot for each optimization problem. In each plot show the performance of all relevant algorithms. For each plot use the parameter setting that gives you the best validation error in Q1 (this refers to the logistic regression probelm). Do not show plots for all parameter settings that you tried in Q1, only for the one that gives you the smallest validation error. Do not include computation of any plot data in the computation of the running time of the algorithm, unless the plot data are computed by the algorithm anyway. Make sure that the plots are clean and use appropriate legends. Note that we should be able to re-run the code and obtain the plots. Marks: 70.\n",
    "\n",
    "### For this question, we will measure the running time of your stochastic sub-gradient method for the sparse dataset news20.binary for the hinge-loss problem. We will not measure the running time of any other combination of algorithm, dataset, problem. You need to implement the stochastic sub-gradient method and encapsulate it in a python class.\n",
    "\n",
    "To make sure your object can be used by our script, your class should have two methods:\n",
    "\n",
    "1. <strong>fit(self, train_data, train_label)</strong>. It will use stochastic sub-gradient method to minimize the hinge loss and store the optimized coefficients (i.e. $x, \\beta$) in the instance. The \"train_data\" and \"train_label\" are similar to the output of \"svm_read_problem\". \n",
    "    * \"train_data\" is a list of $n$ python dictionaries (int -> float), which presents a sparse matrix. The keys (int) and values (float) in the dictionary at train_data[i] are the indices (int) and values (float) of non-zero entries of row $i$. \n",
    "    * \"train_label\" is a list of $n$ integers, it only has <strong>-1s and 1s</strong>. $n$ is the number of samples.  This function returns nothing.\n",
    "\n",
    "\n",
    "2. <strong>predict(self, test_data)</strong>. It will predict the label of the input \"test_data\" by using the coefficients stored in the instance. The \"test_data\" has the same data structure as the \"train_data\" of the \"fit\" function. This function returns a list of <strong>-1s and 1s</strong> (i.e. the prediction of your labels).\n",
    "\n",
    "You can also define other methods to help your programming, we will only call the two methods decribed above.\n",
    "\n",
    "To let us import your class, you need to follow these rules:\n",
    "\n",
    "1. You should name your python file by <strong>a4_[your student ID].py</strong>. For example, if your student id is 12345, then your file name is <strong>a4_12345.py</strong>\n",
    "1. Your object name should be <strong>MyMethod</strong> (it's case sensitive).\n",
    "\n",
    "Any violation of the above requirements will get error in our script and you will get at most 50% of the total score. Your solution will be mainly measured by the runing time of the <strong>fit</strong> function and the accuracy of the <strong>predict</strong> function. For example your method will be called and measured in following pattern:\n",
    "\n",
    "    obj = MyMethod()\n",
    "    st = time.time()\n",
    "    obj.fit(train_data, train_label) # .fit() optimizes the objective and stores coefficients in obj.\n",
    "    running_time = time.time() - st\n",
    "    predict_label = obj.predict(test_data)\n",
    "    accuracy = get_accuracy(predict_label, test_label) # this is a function we use to measure accuracy.\n",
    "Then your accuracy will be measured by <strong>predict_labels</strong>, you don't have to implement \"get_accuracy\". When you finish your implementation, upload the .py file to Learn dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T04:30:11.883017Z",
     "start_time": "2019-11-18T04:30:07.148843Z"
    }
   },
   "outputs": [],
   "source": [
    "A_s, b_s = to_sparse(A, B)\n",
    "a_t, b_t, a_v, b_v = split_train_validate(A_s, b_s)\n",
    "\n",
    "optimizers = {}\n",
    "fvals = {}\n",
    "\n",
    "# Hinge\n",
    "optimizers['hinge_acc_arm'] = Optimizer7(model=HingeLossModel())\n",
    "optimizers['hinge_gd_arm'] = Optimizer6(model=HingeLossModel())\n",
    "optimizers['hinge_sag'] = Optimizer123(model=HingeLossModel())\n",
    "\n",
    "# Smooth Hinge\n",
    "optimizers['s_hinge_acc_arm'] = Optimizer7(model=SmoothHingeLossModel())\n",
    "optimizers['s_hinge_gd_arm'] = Optimizer6(model=SmoothHingeLossModel())\n",
    "optimizers['s_hinge_sag'] = Optimizer123(model=SmoothHingeLossModel()) \n",
    "\n",
    "# Logistic Regression\n",
    "optimizers['log_reg_acc_arm'] = Optimizer7(model=LogisticRegressionModel())\n",
    "optimizers['log_reg_gd_arm'] = Optimizer6(model=LogisticRegressionModel())\n",
    "optimizers['log_reg_sag'] = Optimizer123(model=LogisticRegressionModel()) \n",
    "\n",
    "for name, opt in optimizers.items():\n",
    "    opt.fit(a_t, b_t)\n",
    "    fvals[name] = opt.steps.tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for name, step in fvals.items():\n",
    "    ax.plot(step, label=(name), linewidth=3.0, linestyle='--')\n",
    "\n",
    "ax.legend(prop={'size': 15},loc=\"upper right\")\n",
    "plt.xlabel(\"#Iterations\", fontsize=25)\n",
    "plt.ylabel(f\"Objective\", fontsize=25)\n",
    "ax.grid(linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
