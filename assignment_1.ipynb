{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 1\n",
    "\n",
    "## The assignment is divided into programming and mathematical questions. Both of them are given in this notebook.\n",
    "\n",
    "## Programming questions: I am giving you a template that you can use to write your code. Description of the questions is integrated in the comments.\n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and answers to the mathematical questions on Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Programming Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:40:00.842432Z",
     "start_time": "2019-09-25T23:40:00.037511Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Numpy is useful for handling arrays and dense matrices (a matrix with a lot of nonzeros).\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:40:04.188580Z",
     "start_time": "2019-09-25T23:40:03.694938Z"
    }
   },
   "outputs": [],
   "source": [
    "img = data.stereo_motorcycle()[0]\n",
    "img = rgb2gray(img)*255 # convert to gray and change scale from (0,1) to (0,255).\n",
    "\n",
    "m = img.shape[0]\n",
    "n = img.shape[1]\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: compute the differences operators here. Note that the image is not a square image. Marks: 10/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:40:04.530077Z",
     "start_time": "2019-09-25T23:40:04.427334Z"
    }
   },
   "outputs": [],
   "source": [
    "# You will need these three methods to construct the sparse differences operators.\n",
    "# If you do not use sparse operators you might have memory problems.\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "\n",
    "# Start coding here.\n",
    "\n",
    "# Observation: \n",
    "# The operators must take (m*n)x(m*n) shape to be compatible with the dimension of vectorized image.\n",
    "# The J takes the #columns (n) as dimension, while the I takes #rows (m) as dimension\n",
    "\n",
    "J = diags(diagonals=[\n",
    "    [-1.0] * n,\n",
    "    [1.0] * (n-1)\n",
    "], offsets=[0, 1])\n",
    "\n",
    "I = identity(n=m)\n",
    "\n",
    "# Forward Horizontal Difference\n",
    "D_h = kron(J, I)\n",
    "\n",
    "# Forward Vertical Difference\n",
    "D_v = kron(I, J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: visualize the differences operators applied on the image. Marks: 5/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:40:05.824969Z",
     "start_time": "2019-09-25T23:40:05.131667Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check the figures in the lecture notes on piazza to see an example of what the output should be like. \n",
    "\n",
    "# Start coding here.\n",
    "\n",
    "# vectorize the image, column wise\n",
    "img_vec = (img.T).ravel()\n",
    "\n",
    "# apply D_h to the vectorized image and reshape back to original size\n",
    "img_vec_h = D_h.dot(img_vec)\n",
    "img_vec_h = img_vec_h.reshape((m, n), order='F')\n",
    "\n",
    "# apply D_v to the vectorized image and reshape back to original size\n",
    "img_vec_v = D_v.dot(img_vec)\n",
    "img_vec_v = img_vec_v.reshape((m, n), order='F')\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img_vec_h, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img_vec_v, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T23:40:08.798706Z",
     "start_time": "2019-09-25T23:40:08.211381Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_ = 0\n",
    "standard_deviation = 30\n",
    "dimensions = (m,n)\n",
    "\n",
    "noise = np.random.normal(mean_,standard_deviation,dimensions)\n",
    "\n",
    "noisy_image = img + noise\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(noisy_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quetion 3: Denoise the image. Marks: 20/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method can be used to solve linear systems.\n",
    "from scipy.sparse.linalg import spsolve\n",
    "# This module can be used to convert arrays and matrices from complex to real.\n",
    "from scipy import real\n",
    "\n",
    "# Use the following lambda list to tune the denoising problem.\n",
    "lambda_list = [0.1, 3, 10]\n",
    "\n",
    "# Start coding here. Check figures in Lecture to see an example of what the output should be like.\n",
    "\n",
    "for lamb in lambda_list:\n",
    "    D = D_h + 1j * D_v\n",
    "    A = lamb * real((D.H).dot(D)) + identity(m*n)\n",
    "    # Another implementation of A:\n",
    "    # A = lamb * ((D_h.T).dot(D_h) + (D_v.T).dot(D_v)) + identity(m*n)\n",
    "    b = (noisy_image.T).ravel()\n",
    "    denoised_image = spsolve(A=A, b=b)\n",
    "    denoised_image = denoised_image.reshape((m, n), order='F')\n",
    "    plt.figure(1, figsize=(10, 10))\n",
    "    plt.imshow(denoised_image, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-programming questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: compute the gradient of the denoising objective function $f(x) = \\frac{\\lambda}{2}\\|Dx\\|_2^2 + \\frac{1}{2}\\|x-z_{noisy}\\|_2^2$, where $x$ consists of $n$ coordinates. The gradient has already been given in the class and you are asked to analytically compute and show any steps for deriving the gradient. Marks: 20/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we rewrite the objective function in matrix form:\n",
    "\n",
    "Noting that $z_n^T.x = x^T.z_n$ for any two real vectors $x, z \\in R^n$ , and $||v||_2^2 = v^*.v$, where $*$ is the conjugate transpose operation applied on a complex vector $v$ (which reduces to just a transpose operation for real vectors), and \"$.$\" is the dot product operqation.\n",
    "\n",
    "$f(x)$ can be rewritten as:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "f(x)        &= \\frac{\\lambda}{2}  (D.x)^*.(D.x) + \\frac{1}{2} (x-z_n)^T.(x-z_n)\\\\\n",
    "            &= \\frac{\\lambda}{2}  (x^T.D^*).(D.x) + \\frac{1}{2} (x^T-z_n^T).(x-z_n)\\\\\n",
    "            &= \\frac{\\lambda}{2}  (x^T.A.x) + \\frac{1}{2} (x^T.x - 2x^T.z_n + z_n^T.z_n)\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Where $A = (D^*.D)$. We can see that $A$ is square matrix that also has the property $A^* = A$.\n",
    "\n",
    "Using the following matrix differentiation rules:\n",
    "\n",
    "- $\\nabla (x^T.A.x) = (A^* + A).x = 2A.x$\n",
    "- $\\nabla (x^T.x) = 2x$\n",
    "- $\\nabla (z_n^T.x) = \\nabla (x^T.z_n) = z_n$ where $z_n$ does not depend on $x$.\n",
    "\n",
    "$\\nabla f(x)$ (the gradient) can be obtained as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f(x)        &= \\frac{\\lambda}{2}  (2A.x) + \\frac{1}{2} (2x - 2z_n + 0) \\\\\n",
    "                   &= \\lambda A.x + x - z_n \\\\\n",
    "                   &= [\\lambda A + I]. x - z_n \\\\\n",
    "                   &= [\\lambda (D^*.D) + I]. x - z_n\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 5: compute the second derivative (Hessian matrix) of the denoising objective function $f(x) = \\frac{\\lambda}{2}\\|Dx\\|_2^2 + \\frac{1}{2}\\|x-z_{noisy}\\|_2^2$, where $x$ consists of $n$ coordinates. The Hessian matrix has already been given in the class and you are asked to analytically compute and show any steps for deriving the gradient. Marks: 10/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from Question 4, we know that $\\nabla f(x) = [\\lambda (D^*.D) + I]. x - z_n$ \n",
    "\n",
    "Using the following matrix differentiation rule:\n",
    "- $\\nabla (A.x) = A$\n",
    "\n",
    "$\\nabla^2 f(x)$ (the Hessian) can be obtained as:\n",
    "\n",
    "$$ \n",
    "\\nabla^2 f(x) = \\lambda (D^*.D) + I\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: prove that the denoising objective function $f(x) = \\frac{\\lambda}{2}\\|Dx\\|_2^2 + \\frac{1}{2}\\|x-z_{noisy}\\|_2^2$ is a convex function. Prove this without using the fact that the objective function is differentiable. Marks: 20/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Question 4 we know the matrix form of $f(x)$ is given by:\n",
    "- $f(x) = \\frac{\\lambda}{2}  (x^T.A.x) + \\frac{1}{2} (x^T.x - 2x^T.z_n + z_n^T.z_n)$\n",
    "\n",
    "Where $A = D^*D$ and has the property of being square and $A^* = A$\n",
    "\n",
    "For a function to be convex, Jensen's inequality must hold: $f(\\alpha x + (1-\\alpha) y) \\leq \\alpha f(x) + (1-\\alpha) f(y)$ for any $x, y \\in R^n$. \n",
    "\n",
    "Which means: \n",
    "- $f(\\alpha x + (1-\\alpha) y) - \\alpha f(x) - (1-\\alpha) f(y) \\leq 0$\n",
    "\n",
    "We will use \"LHS\" to refer to the left hand side term of Jensen's inequality.\n",
    "\n",
    "\n",
    "Substituting every $x = (\\alpha x + (1-\\alpha) y)$ in $f(x)$ we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f(\\alpha x + (1-\\alpha) y) &= \\\\  \\frac{\\lambda}{2}  ((\\alpha x + (1-\\alpha) y)^T.A.(\\alpha x + (1-\\alpha) y))\\\\+ \\frac{1}{2} ((\\alpha x + (1-\\alpha) y)^T.(\\alpha x + (1-\\alpha) y) - 2(\\alpha x + (1-\\alpha) y)^T.z_n + z_n^T.z_n)\\\\\n",
    "&=\\\\ \\frac{\\lambda}{2} (\\alpha^2x^T.A.x + \\alpha(1-\\alpha) [y^T.A.x + x^T.A.y] + (1-\\alpha)^2 y^T.A.y)\\\\\n",
    "+ \\frac{1}{2} (\\alpha^2x^T.x + 2\\alpha(1-\\alpha)x^T.y + (1-\\alpha)^2 y^T.y - 2z_n.(\\alpha x^T + (1-\\alpha) y^T) + z_n^T.z_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Subtracting $\\alpha f(x) = \\frac{\\alpha\\lambda}{2}  (x^T.A.x) + \\frac{\\alpha}{2} (x^T.x - 2x^T.z_n + z_n^T.z_n)$ and $(1-\\alpha) f(y) = \\frac{(1-\\alpha)\\lambda}{2}  (y^T.A.y) + \\frac{(1-\\alpha)}{2} (y^T.y - 2y^T.z_n + z_n^T.z_n)$ From $f(\\alpha x + (1-\\alpha) y)$, we obtain the left hand side of Jensen's inequality, which simplifies to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LHS &=\\\\ \\frac{\\lambda}{2} ((x^T.A.x)[\\alpha^2 - \\alpha] + (y^T.A.y)[(1-\\alpha)^2-(1-\\alpha)] + 2\\alpha(1-\\alpha)[x^T.A.y])\\\\ + \\frac{1}{2} ((x^T.x)[\\alpha^2 - \\alpha] + (y^T.y)[(1-\\alpha)^2-(1-\\alpha)] + 2\\alpha(1-\\alpha)x^T.y)\\\\\n",
    "&=\\\\ \\frac{\\alpha^2 - \\alpha}{2} [\\lambda x^T.A.x + x^T.x] + \\frac{(1-\\alpha)^2-(1-\\alpha)}{2} [\\lambda y^T.A.y + y^T.y] + \\alpha(1-\\alpha)[\\lambda x^T.A.y + x^T.y]\\\\\n",
    "&=\\\\ \\frac{\\alpha^2 - \\alpha}{2} x^T.[\\lambda A + I].x + \\frac{(1-\\alpha)^2-(1-\\alpha)}{2} y^T.[\\lambda A + I].y + \\alpha(1-\\alpha)x^T.[\\lambda A + I].y\\\\\n",
    "&=\\\\ \\frac{\\alpha^2 - \\alpha}{2} x^T.G.x + \\frac{(1-\\alpha)^2-(1-\\alpha)}{2} y^T.G.y + \\alpha(1-\\alpha)x^T.G.y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $G=(\\lambda A + I)$\n",
    "\n",
    "\n",
    "Since $A = D^*D$, that means $A$ is positive semidefinte, which makes $G$ also positive semidefinte.\n",
    "\n",
    "Since $0 \\lt \\alpha \\lt 1$, that means $\\alpha^2 < \\alpha$ and $(1-\\alpha)^2 < (1-\\alpha)$\n",
    "\n",
    "Which means that the first and second terms in $LHS$ are negative, it can also be shown their magnitude is greater than the magnitude of the third (potentially positive) term. This can be done by noticing that the $LHS$ takes its greatest value when $x = y$ (otherwise the dot product is not maximized), and in that case $LHS_{max} = -\\alpha^2 x^T.G.x$, which is negative because $G$ is positive semidefinte and $\\alpha \\gt 0$.\n",
    "\n",
    "If the maximum value is negative, then $LHS$ is always negative, which means Jensen's inequality holds for $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: You are given a function $f$ that is twice differentiable and not necessarily convex. Explain why if a point $x$ that satisfies $\\nabla f(x) = 0$ is not necessarily a local minimizer of function $f$. Hint: use the Taylor expansion $f(y)=f(x) + \\nabla f(x)^T (y-x) + (y-x)^T \\nabla^2 f(x) (y-x) + \\mathcal{o}(\\|y-x\\|^2_2)$. Note that $\\mathcal{o}$ denotes the little-o notation and $\\mathcal{o}(\\|y-x\\|^2_2)$ can be intepreted as a function that goes to zero faster than $\\|y-x\\|^2_2$. Marks: 15/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter Example:\n",
    "\n",
    "Let $f(x) = x^3$ (not convex), we find that $\\nabla f(x) = 3x^2$\n",
    "\n",
    "Setting $\\nabla f(x) = 0 \\rightarrow x = 0$ is obviously not a local minimizer since $f(x=0) = 0$ and $f(x-\\epsilon) < 0$ for any $\\epsilon > 0$\n",
    "\n",
    "-------------------\n",
    "Sketch for proof by contradiction. Assume:\n",
    "\n",
    "- (1) $f(x)$ is twice differentiable and not convex $\\rightarrow$ $\\nabla^2 f(x)$ is defined, and not positive semi-definite $\\rightarrow$ $\\nabla^2 f(x) \\ngeq 0 \\rightarrow \\nabla^2 f(x) \\lt 0$ \n",
    "\n",
    "- (2) $\\nabla f(x) = 0$ and $x$ is a local minimizer of $f(x)$ $\\rightarrow$ $\\exists \\epsilon \\gt 0$ such that $f(x) \\leq f(y)$ for any  $y: ||y - x||_{2}^{2} \\leq \\epsilon$\n",
    "\n",
    "\n",
    "Starting with the defintion of a local minimzer from (2):\n",
    "\n",
    "$f(x) \\leq f(y)$\n",
    "\n",
    "Substituting $f(y)$ from the given Taylor expansion:\n",
    "\n",
    "- $f(x) \\leq f(x) + \\nabla f(x)^T (y-x) + (y-x)^T \\nabla^2 f(x) (y-x) + \\mathcal{o}(\\|y-x\\|^2_2)$\n",
    "\n",
    "Substituting  $\\nabla f(x) = 0$\n",
    "\n",
    "- $(y-x)^T \\nabla^2 f(x) (y-x) + \\mathcal{o}(\\|y-x\\|^2_2) \\geq 0$\n",
    "\n",
    "\n",
    "Since $||y - x||_{2}^{2} \\leq \\epsilon$, and $\\epsilon \\gt 0$, $\\epsilon$ can be substituted for $\\mathcal{o}(\\|y-x\\|^2_2)$\n",
    "\n",
    "- $(y-x)^T \\nabla^2 f(x) (y-x) + \\epsilon \\geq 0$ \n",
    "\n",
    "Substituting $(y-x)$ with $z$, we get:\n",
    "\n",
    "- $z^T [\\nabla^2 f(x)] z + \\epsilon \\geq 0$ \n",
    "\n",
    "Since $f(x)$ is not convex, the first term is a negative scalar ($z^T [\\nabla^2 f(x)] z \\lt 0$), because $\\nabla^2 f(x) \\lt 0$, and the magnitude of this negative scalar depends on $f(x)$ and $z$, so for a general non-convex $f(x)$, this inequality does not necessarily hold (i.e. we may get $z^T [\\nabla^2 f(x)] z \\lt -\\epsilon$), which means we reached a contradiction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
